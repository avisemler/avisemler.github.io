<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>


# Exercise solutions and notes from 'Neural Networks and Deep Learning'

I've summarized some of the book in a way that was helpful for me, and also included my solutions to the exercises. It's very notation-heavy because I felt that the book contains sufficient explatation but shows less of the derivations.
- [Exercise solutions and notes from 'Neural Networks and Deep Learning'](#exercise-solutions-and-notes-from-neural-networks-and-deep-learning)
- [Chapter 2: Backpropgation](#chapter-2-backpropgation)
  - [Notation](#notation)
  - [Proof of equations](#proof-of-equations)
    - [Equation 1: error in output layer](#equation-1-error-in-output-layer)
    - [Equation 2: Error in layer $l$ in terms of error in layer $l+1$](#equation-2-error-in-layer-l-in-terms-of-error-in-layer-l1)
    - [Equation 3: Derivative WRT biases](#equation-3-derivative-wrt-biases)
    - [Equation 4: Derivative WRT weights](#equation-4-derivative-wrt-weights)
  - [Exercise: matrix forms](#exercise-matrix-forms)
    - [Equation 1](#equation-1)
    - [Equation 2](#equation-2)
    - [Third exercise](#third-exercise)
  - [Using matrices for minibatches](#using-matrices-for-minibatches)

# Chapter 2: Backpropgation
## Notation

There are $L$ layers.

$w_{jk}^l$ is the weight for connection between $k^{th}$ in layer $l-1$ to $j^{th}$ in layer $l$. Matrix form: $(w^l) _ {jk}:=w_{jk}^l$.


$b_k^l$ and $a_k^l$ are the weights are activations of neuron $k$ in layer $l$. $b^l$ and $a^l$ are vectors.

The matrix form of activations becomes

$$a^l=\sigma(w^la^{l-1}+b^l)$$

with $\sigma$ applied componentwise.

$z^l:=w^la^{l-1}+b^l$.

$C$ is the cost function, assumed to be some nice combination of cost functions for each training example. From now on, $C$ is the cost function for an individual input, and is assumed to depend only on the outputs of the network.

$\delta_l^j:=\frac{\partial C}{\partial z_j^l}$, which can be seen as the error because when it is close to 0 it is close to being a critical point. More like "leverage from changing value of this neuron" - how much this neuron affects the cost.

## Proof of equations

### Equation 1: error in output layer

$$\delta_j^L=\frac{\partial C}{\partial a^L_j}\sigma'(z_j^L)$$

Intuition: the error of neuron $j$ in last layer has two components: the sensitivity of cost to small changes in the activation, and the senitivity of the activation to small changes in the value of $z$.

Proof: Using the definitions:

$$\delta_j^L=\frac{\partial C}{\partial z_j^L}(a_1^L,\dots,a_n^L)=\frac{\partial C}{\partial z_j^L}(\sigma(z_1^L),\dots,\sigma(z_n^L))
$$

The only non-vanishing term in the sum that results when applying the chain rule is:

$$\frac{\partial C}{\partial a_j^L}\frac{\partial a_j^L}{\partial z_j^L}=\frac{\partial C}{\partial a_j^L}\sigma'(z_j^L)$$

because the derivative of activation $j$ with respect to any other z-value is zero.

### Equation 2: Error in layer $l$ in terms of error in layer $l+1$

$$\delta_j^l=\Sigma_k\delta_k^{l+1}w_{kj}^{l+1}\sigma'(z_j^l)$$

This can also be written in matrix form, but I find this less intuitive.

Intuition: the error depends on the error of every neuron
that it is connected to. But if the weight is low, the contribution of the connected neuron is low and vice versa, so we multiply by the weight. Also, if the derivative of sigma is low, then the value of $z$ is less consequential so multiply by that too. 

Proof: viewing $C$ as a function of the $z$ values in the next layer and using chain rule:

$$\delta_j^l=\Sigma_k\frac{\partial C}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^l}=\Sigma_k\delta_k^{l+1}\frac{\partial z_k^{l+1}}{\partial z_j^l}$$

Also, $z_k^{l+1}=b_k^{l+1}+\Sigma_iw_{ki}^l\sigma(z_{i}^l)$. Differentiate this, and substitute to get the result.

### Equation 3: Derivative WRT biases
For a bias $b$ and the corresponding $\delta$:
$$\frac{\partial C}{\partial b}=\delta$$

Intuition: changing the bias by a small amount has the same effect as just directly changing the $z$-value directly.

Proof:
Applying the chain rule, the only non-vanishing term is
$$\frac{\partial C}{\partial b}=\frac{\partial C}{\partial z}\frac{\partial z}{\partial b}=\delta\cdot 1$$

because $\frac{\partial}{\partial b}(b+\Sigma_iw_i\dots)=1$


### Equation 4: Derivative WRT weights
For a weight $w_{jk}^l$:
$$\frac{\partial C}{\partial w_{jk}^l}=a_k^{l-1}\delta_j^i$$.

Intuition: changing a weight effects the cost in proportion with the activation of the neuron that is "feeding into" the edge controlled by that weight.

Proof: identical to previous but differentiate with respect to the weight instead of the bias.


## Exercise: matrix forms

### Equation 1

We view the activation function as $\Sigma:\mathbb{R}^n\rightarrow\mathbb{R}^n$, where $n$ is the size of the $L^{th}$ layer, with $\Sigma(z_1,\dots,z_n):=(\sigma(z_1),\dots,\sigma(z_n))$. Hence, its Jacobian matrix $\Sigma'(z)$ is an
$n\times n$ matrix in the form

$$\begin{pmatrix}
    \sigma'(z_1) & 0 & 0 &\dots \\\
    0 & \sigma'(z_2) & 0 & \dots \\\
    \vdots
\end{pmatrix}$$

Now, we can view $C$ as a function of $z$ values as follows:

$$C\circ \Sigma (z_1^L,\dots, z_n^L)$$

Then, the vector $\delta^L$ is the transpose of the Jacobian:

$$\delta^L=\nabla C\circ \Sigma (z_1^L,\dots, z_n^L)=(\partial C\circ \Sigma (z_1^L,\dots, z_n^L))^T$$

Applying the chain rule:

$$\delta^L=(\partial C(\Sigma (z_1^L,\dots, z_n^L))\Sigma' (z_1^L,\dots, z_n^L))^T$$

Distributing the tranposition:

$$=\nabla C(\Sigma (z_1^L,\dots, z_n^L))\Sigma' (z_1^L,\dots, z_n^L)$$

because $\Sigma'$ is symmetric. This then simplifies to the desired result using $a^L=\Sigma(z^L)$.

### Equation 2

Follows from an analogous modification of the proof of the componentwise version of equation 2.

### Third exercise

This is "telescoping out" the recurrence for $\delta^l$ in terms of $\delta^{l+1}$.

## Using matrices for minibatches

This is a modification of the algorithm to avoid having to iterate over the minibatch, by storing it as the columns of a matrix.

<style>
    body{padding:0 10s%}
</style>
