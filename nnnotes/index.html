<!DOCTYPE html>
<html>
<head>
<title>neural_network_notes.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<h1 id="exercise-solutions-and-notes-from-neural-networks-and-deep-learning">Exercise solutions and notes from 'Neural Networks and Deep Learning'</h1>
<p>I've summarized some of the book in a way that was helpful for me, and also included my solutions to the exercises. It's quite notation-heavy because I felt that the book contains sufficient explanation but shows fewer symbolic derivations, so as an exercise I more explicitly wrote out the formal proofs.</p>
<ul>
<li><a href="#exercise-solutions-and-notes-from-neural-networks-and-deep-learning">Exercise solutions and notes from 'Neural Networks and Deep Learning'</a></li>
<li><a href="#chapter-2-backpropgation">Chapter 2: Backpropgation</a>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#notation">Notation</a></li>
<li><a href="#proof-of-equations">Proof of equations</a>
<ul>
<li><a href="#equation-1-error-in-output-layer">Equation 1: error in output layer</a></li>
<li><a href="#equation-2-error-in-layer-l-in-terms-of-error-in-layer-l1">Equation 2: Error in layer $l$ in terms of error in layer $l+1$</a></li>
<li><a href="#equation-3-derivative-wrt-biases">Equation 3: Derivative WRT biases</a></li>
<li><a href="#equation-4-derivative-wrt-weights">Equation 4: Derivative WRT weights</a></li>
</ul>
</li>
<li><a href="#exercise-matrix-forms">Exercise: matrix forms</a>
<ul>
<li><a href="#equation-1">Equation 1</a></li>
<li><a href="#equation-2">Equation 2</a></li>
<li><a href="#third-exercise">Third exercise</a></li>
</ul>
</li>
<li><a href="#using-matrices-for-minibatches">Using matrices for minibatches</a></li>
</ul>
</li>
<li><a href="#chapter-3-improved-learning">Chapter 3: Improved learning</a>
<ul>
<li><a href="#cross-entropy">Cross-entropy</a>
<ul>
<li><a href="#exercise-show-that-c-is-minimized-when-ay">Exercise: show that $C$ is minimized when $a=y$</a></li>
<li><a href="#exercise-extending-to-networks-with-multiple-neurons-and-layers">Exercise: extending to networks with multiple neurons and layers</a></li>
<li><a href="#exercise-why-x_j-cant-be-eliminated">Exercise: why $x_j$ can't be eliminated</a></li>
</ul>
</li>
<li><a href="#softmax">Softmax</a>
<ul>
<li><a href="#exercise-inverting-softmax">Exercise: inverting softmax</a></li>
<li><a href="#exercise-partial-derivatives-of-log-liklihood-cost-with-softmax">Exercise: Partial derivatives of log-liklihood cost with softmax</a></li>
</ul>
</li>
<li><a href="#overfitting-and-regularization">Overfitting and regularization</a></li>
<li><a href="#weight-initialisation">Weight initialisation</a>
<ul>
<li><a href="#exercise-standard-deviation-of-activation">Exercise: standard deviation of activation</a></li>
<li><a href="#problem-connecting-regularization-and-the-improved-method-of-weight-initialization">Problem: Connecting regularization and the improved method of weight initialization</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="chapter-2-backpropgation">Chapter 2: Backpropgation</h1>
<h2 id="overview">Overview</h2>
<p>We compute gradients of the loss function with respect to each parameter by &quot;pulling gradients
backwards&quot; through the computational graph.</p>
<h2 id="notation">Notation</h2>
<p>There are $L$ layers.</p>
<p>$w_{jk}^l$ is the weight for connection from the $k^{th}$ neuron in layer $l-1$, to the $j^{th}$ neuron in layer $l$. We can store all these values in a matrix per layer: $(w^l) _ {jk}:=w_{jk}^l$. It might seem more intuitive to write instead store the transpose of this matrix, but
the expression for computing a forward pass actually uses this un-transposed version - see equation below.</p>
<p>$b_k^l$ and $a_k^l$ are the weights are activations of neuron $k$ in layer $l$. $b^l$ and $a^l$ are vectors.</p>
<p>The matrix form of activations becomes</p>
<p>$$a^l=\sigma(w^la^{l-1}+b^l)$$</p>
<p>with $\sigma$, the activation function, applied componentwise. This is essentially saying that row $j$ of the matrix $w^l$ specifies what linear combination of the activations $a^{l-1}$ is used to calculate the activation $a^l_j$.</p>
<p>We also define a vector of &quot;raw&quot; activations, without the activation function applied.</p>
<p>$z^l:=w^la^{l-1}+b^l$.</p>
<p>$C$ is the cost function, assumed to be some nice combination of cost functions for each training example. From now on, $C$ is the cost function for an individual input, and is assumed to depend only on the outputs of the network. We make this assumption since if we can calculate gradients $C$ for a single input with respect to the parameters then we can easily calculate the gradients over a batch of example inputs by e.g. taking an average (if $C$ for many inputs is an average).</p>
<p>$\delta_l^j:=\frac{\partial C}{\partial z_j^l}$, which can be seen as the error because when it is close to 0 it is close to being a critical point. More like &quot;leverage from changing value of this neuron&quot; - how much this neuron affects the cost. Useful intermediate quantity for calculating gradients with respect to weights and biases.</p>
<h2 id="proof-of-equations">Proof of equations</h2>
<h3 id="equation-1-error-in-output-layer">Equation 1: error in output layer</h3>
<p>$$\delta_j^L=\frac{\partial C}{\partial a^L_j}\sigma'(z_j^L)$$</p>
<p><strong>Intuition</strong>: The chain rule; the error resulting from the raw activation of neuron $j$ in the last layer is the product of the sensitivity of cost to small changes in the activation, and the senitivity of the activation to small changes in the value of $z$.</p>
<p><strong>Proof</strong>: Using the definitions:</p>
<p>$$\delta_j^L=\frac{\partial C}{\partial z_j^L}(a_1^L,\dots,a_n^L)=\frac{\partial C}{\partial z_j^L}(\sigma(z_1^L),\dots,\sigma(z_n^L))
$$</p>
<p>This is a derivative of a composition of functions, so apply the chain rule to get:</p>
<p>$$=\sum_{i=1}^n\frac{\partial C}{\partial a_i^L}\frac{\partial a_i^L}{\partial z_j^L}$$
The only non-vanishing term in the sum above is:</p>
<p>$$\frac{\partial C}{\partial a_j^L}\frac{\partial a_j^L}{\partial z_j^L}=\frac{\partial C}{\partial a_j^L}\sigma'(z_j^L)$$</p>
<p>because the derivative of an any activation other than $a_j^L$ with respect to z-value $z_j^L$ is zero.</p>
<h3 id="equation-2-error-in-layer-l-in-terms-of-error-in-layer-l1">Equation 2: Error in layer $l$ in terms of error in layer $l+1$</h3>
<p>$$\delta_j^l=\sum_k\delta_k^{l+1}w_{kj}^{l+1}\sigma'(z_j^l)$$</p>
<p>This can also be written in matrix form, but I find this less intuitive.</p>
<p><strong>Intuition</strong>: the error depends on the error of every neuron
that it is connected to. But if the weight is low, the contribution of the connected neuron is low and vice versa, so we multiply by the weight. Also, if the derivative of sigma is low, then the value of $z$ is less consequential so multiply by that too.</p>
<p><strong>Proof</strong>: viewing $C$ as a function of the $z$ values in the next layer and using chain rule:</p>
<p>$$\delta_j^l=\sum_k\frac{\partial C}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^l}=\sum_k\delta_k^{l+1}\frac{\partial z_k^{l+1}}{\partial z_j^l}$$</p>
<p>Also, $z_k^{l+1}=b_k^{l+1}+\sum_iw_{ki}^l\sigma(z_{i}^l)$. Differentiate this, and substitute to get the result.</p>
<h3 id="equation-3-derivative-wrt-biases">Equation 3: Derivative WRT biases</h3>
<p>For a bias $b$ and the corresponding $\delta$:
$$\frac{\partial C}{\partial b}=\delta$$</p>
<p><strong>Intuition</strong>: changing the bias by a small amount has the same effect as just directly changing the $z$-value directly.</p>
<p><strong>Proof</strong>:
Applying the chain rule, the only non-vanishing term is
$$\frac{\partial C}{\partial b}=\frac{\partial C}{\partial z}\frac{\partial z}{\partial b}=\delta\cdot 1$$</p>
<p>because $\frac{\partial}{\partial b}(b+\sum_iw_i\dots)=1$</p>
<h3 id="equation-4-derivative-wrt-weights">Equation 4: Derivative WRT weights</h3>
<p>For a weight $w_{jk}^l$:
$$\frac{\partial C}{\partial w_{jk}^l}=a_k^{l-1}\delta_j^i$$.</p>
<p><strong>Intuition</strong>: changing a weight effects the cost in proportion with the activation of the neuron that is &quot;feeding into&quot; the edge controlled by that weight.</p>
<p><strong>Proof</strong>: identical to previous but differentiate with respect to the weight instead of the bias.</p>
<h2 id="exercise-matrix-forms">Exercise: matrix forms</h2>
<h3 id="equation-1">Equation 1</h3>
<p>We view the activation function as $\Sigma:\mathbb{R}^n\rightarrow\mathbb{R}^n$, where $n$ is the size of the $L^{th}$ layer, with $\Sigma(z_1,\dots,z_n):=(\sigma(z_1),\dots,\sigma(z_n))$. Hence, its Jacobian matrix $\Sigma'(z)$ is an
$n\times n$ matrix in the form</p>
<p>$$\begin{pmatrix}
\sigma'(z_1) &amp; 0 &amp; 0 &amp;\dots \\
0 &amp; \sigma'(z_2) &amp; 0 &amp; \dots \\
\vdots
\end{pmatrix}$$</p>
<p>Now, we can view $C$ as a function of $z$ values as follows:</p>
<p>$$C\circ \Sigma (z_1^L,\dots, z_n^L)$$</p>
<p>Then, the vector $\delta^L$ is the transpose of the Jacobian:</p>
<p>$$\delta^L=\nabla C\circ \Sigma (z_1^L,\dots, z_n^L)=(\partial C\circ \Sigma (z_1^L,\dots, z_n^L))^T$$</p>
<p>Applying the chain rule:</p>
<p>$$\delta^L=(\partial C(\Sigma (z_1^L,\dots, z_n^L))\Sigma' (z_1^L,\dots, z_n^L))^T$$</p>
<p>Distributing the tranposition using $(AB)^T=B^TA^T$:</p>
<p>$$= \Sigma' (z_1^L,\dots, z_n^L)\nabla C(\Sigma (z_1^L,\dots, z_n^L))$$</p>
<p>because $\Sigma'$ is symmetric. This then simplifies to the desired result using $a^L=\Sigma(z^L)$.</p>
<h3 id="equation-2">Equation 2</h3>
<p>Follows from an analogous modification of the proof of the componentwise version of equation 2.</p>
<h3 id="third-exercise">Third exercise</h3>
<p>This is &quot;telescoping out&quot; the recurrence for $\delta^l$ in terms of $\delta^{l+1}$.</p>
<h2 id="using-matrices-for-minibatches">Using matrices for minibatches</h2>
<p>This is a modification of the algorithm to avoid having to iterate over the minibatch, by storing it as the columns of a matrix.</p>
<p><strong>Input</strong>: The $m$ training examples $x_1,\dots,x_m$ that belong to the minibatch. Store these as an $r\times m$ matrix of activations (where $r$ is the size of each $x_i$):</p>
<p>$$A=\begin{pmatrix}
x_1, &amp; \dots ,&amp; x_m
\end{pmatrix}$$</p>
<p>Create a similar matrix $b^l$ to store biases, with duplicated columns.</p>
<p><strong>Feedforward</strong>: In this stage of the algorithm, we want to update $A$ once for each layer in the network. Multiplying $A$ by the weights matrices has the desired effect.</p>
<p>For $2\leq l \leq L$:</p>
<p>$A\gets w^lA, A\gets A+b^l, Z^l\gets A, A\gets\sigma(A)$</p>
<p><strong>Error from the output layer</strong>:</p>
<p>$\delta^L\gets \nabla C(A)\odot\sigma'(Z^L)$</p>
<p>applying $\sigma'$ elementwise.</p>
<p><strong>Backpropogation</strong>: Again, all operations can be applied as usual on the larger matrix to obtain $\delta^1,\dots,\delta^{L-1}$.</p>
<p><strong>Gradient descent</strong>:</p>
<p>In this stage, we must average the rows of the $\delta$s to find the size of the adjustements to be made to the weights and biases. I think the most performative way to do this is to iterate, or maybe multiplying with a matrix of 1s is easier (???).</p>
<style>
    body{padding:0 10%}
</style>
<h1 id="chapter-3-improved-learning">Chapter 3: Improved learning</h1>
<h2 id="cross-entropy">Cross-entropy</h2>
<p>The version used here for a layer of sigmoid neurons differs from the usual information theoretic definition of cross-entropy (because the outputs do not represent a probability distribution over all possible classifications, so instead consider each neuron as being a Bernoulli distribution). The problem being solved is that with quadratic cost, there is a $\sigma'$ term in the derivative meaning that learning is slow when an activation is very high or low.</p>
<p>$$C:=-\frac{1}{n}\sum_x y\log(a)+(1-y)\log(1-a)$$</p>
<p>with the sum over $x$ to account for each example, and $y$ being the desired output. Using the chain rule and manipulation:</p>
<p>$$\frac{\partial C}{\partial w_j}=x_j(a-y) \text{  (averaged over training examples)}$$</p>
<p>which lacks a $\sigma'$ term.</p>
<h3 id="exercise-show-that-c-is-minimized-when-ay">Exercise: show that $C$ is minimized when $a=y$</h3>
<p>Consider one training example:</p>
<p>$$\frac{dC}{da}=-\frac{y}{a}+\frac{1-y}{1-a}$$</p>
<p>which is clearly 0 when $a=y$. It can be further verified that this stationary point is a minimum by differentiating again.</p>
<h3 id="exercise-extending-to-networks-with-multiple-neurons-and-layers">Exercise: extending to networks with multiple neurons and layers</h3>
<p>The crux of the exercise is to show that for a single training example $\delta^L=a^L-y$. Consider components of $\delta^L$:</p>
<p>$$\delta^L_j=\frac{\partial C}{\partial z^L_j}=\frac{\partial}{\partial z_j^L}-\sum_jy_j\log(a_j^L)+(1-y_j)\log(1-a_j^L)$$</p>
<h3 id="exercise-why-xj-cant-be-eliminated">Exercise: why $x_j$ can't be eliminated</h3>
<p>Because the derivative of the cost can't depend on $x_j$ - there are many different values of $x_j$ that would result in the same cost.</p>
<h2 id="softmax">Softmax</h2>
<p>Using softmax for the output layer means that the activations form a probability distribution (sum to 1).</p>
<p>Enables use of log-liklihood cost function without slowdown:</p>
<p>$$C=-\log a_c^l$$</p>
<p>where $c$ is the index of the output neuron that we desire to be fully activated (using one-hot encoding).</p>
<h3 id="exercise-inverting-softmax">Exercise: inverting softmax</h3>
<p>$$\log(a_j)=z_j-\log\left(\sum_i e^{z_i}\right)$$</p>
<p>as required.</p>
<h3 id="exercise-partial-derivatives-of-log-liklihood-cost-with-softmax">Exercise: Partial derivatives of log-liklihood cost with softmax</h3>
<p>It suffices to find an expression for $\delta_j^L$ and then the results follow. The key difference now is that activations depend on all other $z$-values (so $\delta$ doesn't vanish when $j \neq c$):</p>
<p>$$\delta^L_j=\sum_i \frac{\partial a_i^L}{\partial z_j^L}\frac{\partial C}{\partial a_i^L} $$</p>
<p>The non-vanishing term is $\frac{\partial C}{\partial a_c^L}\frac{a_c^L}{z_J^L}=\frac{-1}{a_c^L}\frac{a_c^L}{z_J^L}=$. Then evaluate the second derivative.</p>
<h2 id="overfitting-and-regularization">Overfitting and regularization</h2>
<p>The cost function can be reduced in two ways - memorizing specifics of the training data, or generalizing (correctly or incorrectly). We want to tweak the search and space such that more of the second kind occurs.</p>
<p>One way to do this is to stop training early, once the performance on the validation data is no longer improving. Scaling up the amount of data used can also make it harder to improve the cost via overfitting.</p>
<p>Weight decay (or L2 regularisation) is another approach to make it more likely that the network will generalise: it adds on a $\frac{\lambda}{2n}\sum_ww^2$ term to the cost. This encourages the network to have fewer high weights and hence to learn lower complexity functions which are more likely to be general explanations as opposed to memorisations of specifics: it can't &quot;afford&quot; to overreact to single datapoints.</p>
<p>L1 regularisation adds on the $\ell_1$ norm of the weights to the cost - which results in a few very high activation weights, as the reduction is constant.</p>
<p>Dropout randomly excludes a subset of the neurons when performing SGD. This can be seen as averaging the outputs of many different networks. There is also an effect of forcing each neuron to learn a robustly useful feature, that does not rely on the presence of other neurons to be useful.</p>
<p>Another approach is to generate more training examples by applying small perturbations to current ones.</p>
<h2 id="weight-initialisation">Weight initialisation</h2>
<p>If we use a normal distribution with constant variance, then in expectation the magnitudes  of activations will be high - meaning that learning slowdown occurs with sigmoid activation. A solution is to divide the variance by the size of the layer to reduce expected activation magnitudes.</p>
<h3 id="exercise-standard-deviation-of-activation">Exercise: standard deviation of activation</h3>
<p>$$\text{Var}(b+\sum_{i=1}^{500}w_i)=\text{Var}(b)+\text{Var}(\sum_{i=1}^{500}w_i)=1+500\frac{1}{1000}=\frac{3}{2}$$</p>
<p>Square root to get desired standard deviation.</p>
<h3 id="problem-connecting-regularization-and-the-improved-method-of-weight-initialization">Problem: Connecting regularization and the improved method of weight initialization</h3>
<p>If the old approach to weight initialisation is used, then in early epochs the weights will be high and the fastest way to decrease the cost locally will be to reduce the magnitude of the weights instead of achieving better classification accuracy.</p>

</body>
</html>
