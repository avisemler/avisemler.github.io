<!DOCTYPE html>
<html>
<head>
<title>backprop.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 10%;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 11%;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<h1 id="exercise-solutions-and-notes-from-neural-networks-and-deep-learning">Exercise solutions and notes from 'Neural Networks and Deep Learning'</h1>
<p>I've summarized some of the book in a way that was helpful for me, and also included my solutions to the exercises. It's very notation-heavy because I felt that the book contains sufficient explatation but shows less of the derivations.</p>
<ul>
<li><a href="#exercise-solutions-and-notes-from-neural-networks-and-deep-learning">Exercise solutions and notes from 'Neural Networks and Deep Learning'</a></li>
<li><a href="#chapter-2-backpropgation">Chapter 2: Backpropgation</a>
<ul>
<li><a href="#notation">Notation</a></li>
<li><a href="#proof-of-equations">Proof of equations</a>
<ul>
<li><a href="#equation-1-error-in-output-layer">Equation 1: error in output layer</a></li>
<li><a href="#equation-2-error-in-layer-l-in-terms-of-error-in-layer-l1">Equation 2: Error in layer $l$ in terms of error in layer $l+1$</a></li>
<li><a href="#equation-3-derivative-wrt-biases">Equation 3: Derivative WRT biases</a></li>
<li><a href="#equation-4-derivative-wrt-weights">Equation 4: Derivative WRT weights</a></li>
</ul>
</li>
<li><a href="#exercise-matrix-forms">Exercise: matrix forms</a>
<ul>
<li><a href="#equation-1">Equation 1</a></li>
<li><a href="#equation-2">Equation 2</a></li>
<li><a href="#third-exercise">Third exercise</a></li>
</ul>
</li>
<li><a href="#using-matrices-for-minibatches">Using matrices for minibatches</a></li>
</ul>
</li>
</ul>
<h1 id="chapter-2-backpropgation">Chapter 2: Backpropgation</h1>
<h2 id="notation">Notation</h2>
<p>There are $L$ layers.</p>
<p>$w_{jk}^l$ is the weight for connection between $k^{th}$ in layer $l-1$ to $j^{th}$ in layer $l$. Matrix form: $(w^l) _ {jk}:=w_{jk}^l$.</p>
<p>$b_k^l$ and $a_k^l$ are the weights are activations of neuron $k$ in layer $l$. $b^l$ and $a^l$ are vectors.</p>
<p>The matrix form of activations becomes</p>
<p>$$a^l=\sigma(w^la^{l-1}+b^l)$$</p>
<p>with $\sigma$ applied componentwise.</p>
<p>$z^l:=w^la^{l-1}+b^l$.</p>
<p>$C$ is the cost function, assumed to be some nice combination of cost functions for each training example. From now on, $C$ is the cost function for an individual input, and is assumed to depend only on the outputs of the network.</p>
<p>$\delta_l^j:=\frac{\partial C}{\partial z_j^l}$, which can be seen as the error because when it is close to 0 it is close to being a critical point. More like &quot;leverage from changing value of this neuron&quot; - how much this neuron affects the cost.</p>
<h2 id="proof-of-equations">Proof of equations</h2>
<h3 id="equation-1-error-in-output-layer">Equation 1: error in output layer</h3>
<p>$$\delta_j^L=\frac{\partial C}{\partial a^L_j}\sigma'(z_j^L)$$</p>
<p>Intuition: the error of neuron $j$ in last layer has two components: the sensitivity of cost to small changes in the activation, and the senitivity of the activation to small changes in the value of $z$.</p>
<p>Proof: Using the definitions:</p>
<p>$$\delta_j^L=\frac{\partial C}{\partial z_j^L}(a_1^L,\dots,a_n^L)=\frac{\partial C}{\partial z_j^L}(\sigma(z_1^L),\dots,\sigma(z_n^L))
$$</p>
<p>The only non-vanishing term in the sum that results when applying the chain rule is:</p>
<p>$$\frac{\partial C}{\partial a_j^L}\frac{\partial a_j^L}{\partial z_j^L}=\frac{\partial C}{\partial a_j^L}\sigma'(z_j^L)$$</p>
<p>because the derivative of activation $j$ with respect to any other z-value is zero.</p>
<h3 id="equation-2-error-in-layer-l-in-terms-of-error-in-layer-l1">Equation 2: Error in layer $l$ in terms of error in layer $l+1$</h3>
<p>$$\delta_j^l=\Sigma_k\delta_k^{l+1}w_{kj}^{l+1}\sigma'(z_j^l)$$</p>
<p>This can also be written in matrix form, but I find this less intuitive.</p>
<p>Intuition: the error depends on the error of every neuron
that it is connected to. But if the weight is low, the contribution of the connected neuron is low and vice versa, so we multiply by the weight. Also, if the derivative of sigma is low, then the value of $z$ is less consequential so multiply by that too.</p>
<p>Proof: viewing $C$ as a function of the $z$ values in the next layer and using chain rule:</p>
<p>$$\delta_j^l=\Sigma_k\frac{\partial C}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^l}=\Sigma_k\delta_k^{l+1}\frac{\partial z_k^{l+1}}{\partial z_j^l}$$</p>
<p>Also, $z_k^{l+1}=b_k^{l+1}+\Sigma_iw_{ki}^l\sigma(z_{i}^l)$. Differentiate this, and substitute to get the result.</p>
<h3 id="equation-3-derivative-wrt-biases">Equation 3: Derivative WRT biases</h3>
<p>For a bias $b$ and the corresponding $\delta$:
$$\frac{\partial C}{\partial b}=\delta$$</p>
<p>Intuition: changing the bias by a small amount has the same effect as just directly changing the $z$-value directly.</p>
<p>Proof:
Applying the chain rule, the only non-vanishing term is
$$\frac{\partial C}{\partial b}=\frac{\partial C}{\partial z}\frac{\partial z}{\partial b}=\delta\cdot 1$$</p>
<p>because $\frac{\partial}{\partial b}(b+\Sigma_iw_i\dots)=1$</p>
<h3 id="equation-4-derivative-wrt-weights">Equation 4: Derivative WRT weights</h3>
<p>For a weight $w_{jk}^l$:
$$\frac{\partial C}{\partial w_{jk}^l}=a_k^{l-1}\delta_j^i$$.</p>
<p>Intuition: changing a weight effects the cost in proportion with the activation of the neuron that is &quot;feeding into&quot; the edge controlled by that weight.</p>
<p>Proof: identical to previous but differentiate with respect to the weight instead of the bias.</p>
<h2 id="exercise-matrix-forms">Exercise: matrix forms</h2>
<h3 id="equation-1">Equation 1</h3>
<p>We view the activation function as $\Sigma:\mathbb{R}^n\rightarrow\mathbb{R}^n$, where $n$ is the size of the $L^{th}$ layer, with $\Sigma(z_1,\dots,z_n):=(\sigma(z_1),\dots,\sigma(z_n))$. Hence, its Jacobian matrix $\Sigma'(z)$ is an
$n\times n$ matrix in the form</p>
<p>$$\begin{pmatrix}
\sigma'(z_1) &amp; 0 &amp; 0 &amp;\dots \\
0 &amp; \sigma'(z_2) &amp; 0 &amp; \dots \\
\vdots
\end{pmatrix}$$</p>
<p>Now, we can view $C$ as a function of $z$ values as follows:</p>
<p>$$C\circ \Sigma (z_1^L,\dots, z_n^L)$$</p>
<p>Then, the vector $\delta^L$ is the transpose of the Jacobian:</p>
<p>$$\delta^L=\nabla C\circ \Sigma (z_1^L,\dots, z_n^L)=(\partial C\circ \Sigma (z_1^L,\dots, z_n^L))^T$$</p>
<p>Applying the chain rule:</p>
<p>$$\delta^L=(\partial C(\Sigma (z_1^L,\dots, z_n^L))\Sigma' (z_1^L,\dots, z_n^L))^T$$</p>
<p>Distributing the tranposition:</p>
<p>$$=\nabla C(\Sigma (z_1^L,\dots, z_n^L))\Sigma' (z_1^L,\dots, z_n^L)$$</p>
<p>because $\Sigma'$ is symmetric. This then simplifies to the desired result using $a^L=\Sigma(z^L)$.</p>
<h3 id="equation-2">Equation 2</h3>
<p>Follows from an analogous modification of the proof of the componentwise version of equation 2.</p>
<h3 id="third-exercise">Third exercise</h3>
<p>This is &quot;telescoping out&quot; the recurrence for $\delta^l$ in terms of $\delta^{l+1}$.</p>
<h2 id="using-matrices-for-minibatches">Using matrices for minibatches</h2>
<p>This is a modification of the algorithm to avoid having to iterate over the minibatch, by storing it as the columns of a matrix.</p>

</body>
</html>
